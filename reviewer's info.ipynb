{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#리뷰어 key unique 개수 확인\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5982"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"review.pickle\", \"rb\") as g:\n",
    "    review = pickle.load(g) # 수집한 리뷰어 key 불러오기\n",
    "\n",
    "review['리뷰자'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = review[['리뷰자']].reset_index(drop=True)\n",
    "df.drop_duplicates(inplace=True) #리뷰자 고유값만 남기기\n",
    "repeat_values = df['리뷰자'].repeat(20) #20번씩 반복\n",
    "df = pd.DataFrame(repeat_values, columns=['리뷰자'])\n",
    "\n",
    "df['type1'] = np.nan; df['type2'] = np.nan; df['type3'] = np.nan; df['type4'] = np.nan\n",
    "df['브랜드'] = np.nan; df['상품'] = np.nan; df['평점'] = np.nan; df['작성일자'] = np.nan\n",
    "df['본문'] = np.nan\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free-proxy get\n",
    "proxy = FreeProxy(rand = True, https = True).get()\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_experimental_option(\"detach\", True)\n",
    "chrome_options.add_experimental_option(\"excludeSwitches\",[\"enable-logging\"])\n",
    "chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "#proxy server 사용\n",
    "proxy_server = proxy.split('//')[1]\n",
    "chrome_options.add_argument(f'--proxy-server={proxy_server}')\n",
    "service = Service(executable_path=ChromeDriverManager().install())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번째 크롤링 완료\n",
      "1000 번째 크롤링 완료\n",
      "2000 번째 크롤링 완료\n",
      "3000 번째 크롤링 완료\n",
      "4000 번째 크롤링 완료\n",
      "5000 번째 크롤링 완료\n",
      "6000 번째 크롤링 완료\n",
      "7000 번째 크롤링 완료\n",
      "8000 번째 크롤링 완료\n"
     ]
    }
   ],
   "source": [
    "#반복문으로 구현하기\n",
    "base_url = 'https://www.oliveyoung.co.kr/store/mypage/getReviewerProfile.do?key={}'\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# 유저 반복문 추가하기\n",
    "for m,n in enumerate(df['리뷰자']):\n",
    "    if pd.isnull(df.loc[m, '상품']): #크롤링 안 된 부분부터\n",
    "        if m % 20 == 0: # 인덱스가 20의 배수인 경우만 \n",
    "            url = base_url.format(n)\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                driver.implicitly_wait(5)\n",
    "                for _ in range(2): #스크롤 2번 실행\n",
    "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    time.sleep(1)\n",
    "\n",
    "                resp = driver.page_source #현재 페이지 파싱\n",
    "                soup = BeautifulSoup(resp, \"html.parser\")\n",
    "                for i in range(0,20): #상품 20개 크롤링\n",
    "                    try:\n",
    "                        type = soup.find('ul','profile-keyword-list').find_all('li','list-item')\n",
    "                        if len(type) == 4: #타입 정보가 전부 있을 경우\n",
    "                            df.loc[m+i,'type1'] = type[0].get_text()\n",
    "                            df.loc[m+i,'type2'] = type[1].get_text()\n",
    "                            df.loc[m+i,'type3'] = type[2].get_text()\n",
    "                            df.loc[m+i,'type4'] = type[3].get_text()\n",
    "                        else: #타입 정보가 일부만 있을 경우\n",
    "                            for j in range(len(type)):\n",
    "                                df.loc[m+i, 'type{}'.format(j+1)] = type[j].get_text()\n",
    "                    except AttributeError: #타입 정보가 없을 경우\n",
    "                        df.iloc[m+i, 1:5] = np.nan #결측값으로 채우기\n",
    "                    try:\n",
    "                        df.loc[m+i, '브랜드'] = soup.select('p.rw-box-figcaption__brand')[i].get_text()\n",
    "                        df.loc[m+i, '상품'] = soup.select('p.rw-box-figcaption__name')[i].get_text()\n",
    "                        df.loc[m+i,'평점'] = soup.select('span.point')[i].get_text()\n",
    "                        df.loc[m+i, '작성일자'] = soup.select('span.review_point_text')[i].get_text()\n",
    "                        df.loc[m+i, '본문'] = soup.select('p.rw-box__description')[i].get_text()\n",
    "                        df.loc[m+i, '랭킹'] = soup.select('div.profile-badge.on')[0].get_text()\n",
    "                    except IndexError: #리뷰한 상품 개수가 20개보다 적을 경우\n",
    "                        df.iloc[m+i, 5:] = np.nan #결측값으로 채우기\n",
    "                if m % 1000 == 0:\n",
    "                    print(m, \"번째 크롤링 완료\")\n",
    "            except: #에러 발생 시 멈춤\n",
    "                print(m, \"번째에서 에러 발생\")\n",
    "                break\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#크롤링된 경우의 데이터프레임 생성\n",
    "crawl_done = df[(df['상품'].isna() == False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#프로필 비공개인 경우 : 크롤링된 경우 이전에 한해서\n",
    "private_temp = df.iloc[:list(crawl_done.tail(1).index)[0]+1, :].copy()\n",
    "private = private_temp[private_temp['상품'].isna() == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n"
     ]
    }
   ],
   "source": [
    "#크롤링된 경우 + 프로필 비공개인 경우 비율\n",
    "print((crawl_done.shape[0] + private.shape[0]) / df.shape[0] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#크롤링된 경우 + 프로필 비공개인 경우 저장\n",
    "save_df = df.iloc[:list(crawl_done.tail(1).index)[0]+1, :].copy()\n",
    "\n",
    "#이전에 크롤링한 결과랑 concat\n",
    "total_df = pd.concat([previous, save_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>리뷰자</th>\n",
       "      <th>type1</th>\n",
       "      <th>type2</th>\n",
       "      <th>type3</th>\n",
       "      <th>type4</th>\n",
       "      <th>브랜드</th>\n",
       "      <th>상품</th>\n",
       "      <th>평점</th>\n",
       "      <th>작성일자</th>\n",
       "      <th>본문</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [리뷰자, type1, type2, type3, type4, 브랜드, 상품, 평점, 작성일자, 본문]\n",
       "Index: []"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 겹치는 거 있는지 확인\n",
    "previous.merge(save_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링한 결과 저장\n",
    "with open(\"crawl_done.pickle\", \"wb\") as g:\n",
    "    pickle.dump(total_df, g)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
